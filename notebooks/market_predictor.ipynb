{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "\n",
    "#from tensorflow.keras.models import Model, Sequential\n",
    "#from tensorflow.keras.layers import Dense, CuDNNLSTM, Activation, Dropout\n",
    "#from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "\n",
    "from pgtaa.config import *\n",
    "from pgtaa.core.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = TimeseriesGenerator(data, targets,\n",
    "                               length=10, sampling_rate=2,\n",
    "                               batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iShares Russell 1000 Value ETF (IWD)</th>\n",
       "      <th>iShares Russell 1000 Growth ETF (IWF)</th>\n",
       "      <th>iShares Russell 2000 Growth ETF (IWO)</th>\n",
       "      <th>iShares Russell 2000 Value ETF (IWN)</th>\n",
       "      <th>iShares MSCI EAFE ETF (EFA)</th>\n",
       "      <th>iShares TIPS Bond ETF (TIP)</th>\n",
       "      <th>SPDR Gold Trust (GLD)</th>\n",
       "      <th>Vanguard REIT ETF (VNQ)</th>\n",
       "      <th>USD/EUR</th>\n",
       "      <th>CNY/USD</th>\n",
       "      <th>...</th>\n",
       "      <th>Civilian Unemployment Ratet</th>\n",
       "      <th>Consumer Price Index for All Urban Consumers: All Items</th>\n",
       "      <th>Industrial Production Index</th>\n",
       "      <th>Federal Debt: Total Public Debt as Percent of Gross Domestic Product</th>\n",
       "      <th>Personal Consumption Expenditures</th>\n",
       "      <th>Effective Federal Funds Rate</th>\n",
       "      <th>Primary Credit Rate</th>\n",
       "      <th>Interest Rates Discount Rate for United States</th>\n",
       "      <th>10-Year Treasury Constant Maturity Rate</th>\n",
       "      <th>All Employees: Total Nonfarm Payrolls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-10</th>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>-0.003177</td>\n",
       "      <td>1.3109</td>\n",
       "      <td>8.2765</td>\n",
       "      <td>...</td>\n",
       "      <td>5.327273</td>\n",
       "      <td>191.818182</td>\n",
       "      <td>98.971655</td>\n",
       "      <td>60.919206</td>\n",
       "      <td>8536.6602</td>\n",
       "      <td>2.34</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.318182</td>\n",
       "      <td>4.29</td>\n",
       "      <td>132864.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-11</th>\n",
       "      <td>-0.006302</td>\n",
       "      <td>-0.006832</td>\n",
       "      <td>-0.011296</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.001923</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.006198</td>\n",
       "      <td>-0.010124</td>\n",
       "      <td>1.3161</td>\n",
       "      <td>8.2765</td>\n",
       "      <td>...</td>\n",
       "      <td>5.331818</td>\n",
       "      <td>191.854545</td>\n",
       "      <td>99.001864</td>\n",
       "      <td>60.915506</td>\n",
       "      <td>8538.9444</td>\n",
       "      <td>2.35</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.329545</td>\n",
       "      <td>4.26</td>\n",
       "      <td>132875.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-12</th>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.001921</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>-0.004734</td>\n",
       "      <td>1.3281</td>\n",
       "      <td>8.2765</td>\n",
       "      <td>...</td>\n",
       "      <td>5.336364</td>\n",
       "      <td>191.890909</td>\n",
       "      <td>99.032073</td>\n",
       "      <td>60.911805</td>\n",
       "      <td>8541.2286</td>\n",
       "      <td>2.36</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3.340909</td>\n",
       "      <td>4.25</td>\n",
       "      <td>132886.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            iShares Russell 1000 Value ETF (IWD)  \\\n",
       "2005-01-10                              0.004013   \n",
       "2005-01-11                             -0.006302   \n",
       "2005-01-12                              0.002320   \n",
       "\n",
       "            iShares Russell 1000 Growth ETF (IWF)  \\\n",
       "2005-01-10                               0.006250   \n",
       "2005-01-11                              -0.006832   \n",
       "2005-01-12                               0.004378   \n",
       "\n",
       "            iShares Russell 2000 Growth ETF (IWO)  \\\n",
       "2005-01-10                               0.009982   \n",
       "2005-01-11                              -0.011296   \n",
       "2005-01-12                               0.001428   \n",
       "\n",
       "            iShares Russell 2000 Value ETF (IWN)  iShares MSCI EAFE ETF (EFA)  \\\n",
       "2005-01-10                              0.010000                     0.004509   \n",
       "2005-01-11                             -0.009085                    -0.001923   \n",
       "2005-01-12                              0.001921                     0.008031   \n",
       "\n",
       "            iShares TIPS Bond ETF (TIP)  SPDR Gold Trust (GLD)  \\\n",
       "2005-01-10                     0.000668               0.002629   \n",
       "2005-01-11                     0.000953               0.006198   \n",
       "2005-01-12                     0.003523               0.009240   \n",
       "\n",
       "            Vanguard REIT ETF (VNQ)  USD/EUR  CNY/USD  \\\n",
       "2005-01-10                -0.003177   1.3109   8.2765   \n",
       "2005-01-11                -0.010124   1.3161   8.2765   \n",
       "2005-01-12                -0.004734   1.3281   8.2765   \n",
       "\n",
       "                            ...                    \\\n",
       "2005-01-10                  ...                     \n",
       "2005-01-11                  ...                     \n",
       "2005-01-12                  ...                     \n",
       "\n",
       "            Civilian Unemployment Ratet  \\\n",
       "2005-01-10                     5.327273   \n",
       "2005-01-11                     5.331818   \n",
       "2005-01-12                     5.336364   \n",
       "\n",
       "            Consumer Price Index for All Urban Consumers: All Items  \\\n",
       "2005-01-10                                         191.818182         \n",
       "2005-01-11                                         191.854545         \n",
       "2005-01-12                                         191.890909         \n",
       "\n",
       "            Industrial Production Index  \\\n",
       "2005-01-10                    98.971655   \n",
       "2005-01-11                    99.001864   \n",
       "2005-01-12                    99.032073   \n",
       "\n",
       "            Federal Debt: Total Public Debt as Percent of Gross Domestic Product  \\\n",
       "2005-01-10                                          60.919206                      \n",
       "2005-01-11                                          60.915506                      \n",
       "2005-01-12                                          60.911805                      \n",
       "\n",
       "            Personal Consumption Expenditures  Effective Federal Funds Rate  \\\n",
       "2005-01-10                          8536.6602                          2.34   \n",
       "2005-01-11                          8538.9444                          2.35   \n",
       "2005-01-12                          8541.2286                          2.36   \n",
       "\n",
       "            Primary Credit Rate  \\\n",
       "2005-01-10                 3.25   \n",
       "2005-01-11                 3.25   \n",
       "2005-01-12                 3.25   \n",
       "\n",
       "            Interest Rates Discount Rate for United States  \\\n",
       "2005-01-10                                        3.318182   \n",
       "2005-01-11                                        3.329545   \n",
       "2005-01-12                                        3.340909   \n",
       "\n",
       "            10-Year Treasury Constant Maturity Rate  \\\n",
       "2005-01-10                                     4.29   \n",
       "2005-01-11                                     4.26   \n",
       "2005-01-12                                     4.25   \n",
       "\n",
       "            All Employees: Total Nonfarm Payrolls  \n",
       "2005-01-10                          132864.545455  \n",
       "2005-01-11                          132875.636364  \n",
       "2005-01-12                          132886.727273  \n",
       "\n",
       "[3 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN_CSV, index_col=0, parse_dates=True)\n",
    "df.iloc[:, :8] = df.iloc[:,:8].pct_change(1)\n",
    "df.dropna(inplace=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = StandardScaler().fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70890834],\n",
       "       [-0.69316197],\n",
       "       [-0.66428075],\n",
       "       [-0.69967173],\n",
       "       [-0.54147367],\n",
       "       [ 2.2091984 ],\n",
       "       [ 1.53419518],\n",
       "       [-0.43589713]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=1)\n",
    "pcc = pca.fit_transform(np.cov(train[:,:8].T))\n",
    "pcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.iloc[:,:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-fccf455c3824>, line 385)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-fccf455c3824>\"\u001b[0;36m, line \u001b[0;32m385\u001b[0m\n\u001b[0;31m    benchmark = pass\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class PortfolioEnv(object):\n",
    "    \"\"\"\n",
    "    Creates a portfolio environment, similar to the TensorForce or Gym environment:\n",
    "    Gym: https://github.com/openai/gym/blob/522c2c532293399920743265d9bc761ed18eadb3/gym/core.py\n",
    "    TensorForce: https://github.com/reinforceio/tensorforce/blob/master/tensorforce/environments/environment.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            data: np.ndarray,\n",
    "            assets: list,\n",
    "            nb_assets: int=10,\n",
    "            horizon: int=20,\n",
    "            action_space: str='unbounded',\n",
    "            window_size: int=100,\n",
    "            portfolio_value: float=1000.0,\n",
    "            risk_aversion: float=1.0,\n",
    "            num_actions: int=11,\n",
    "            cost_buying: float=0.025,\n",
    "            cost_selling: float=0.025,\n",
    "            cost_fix: float=0.0,\n",
    "            predictor=None,\n",
    "            optimized: bool=True,\n",
    "            action_type: str='signal_softmax',\n",
    "            scaler=None,\n",
    "            standardize: bool=True,\n",
    "            action_bounds: tuple=(0.0, 1.0),\n",
    "            discrete_states: bool=False,\n",
    "            state_labels: tuple=(50,),\n",
    "            episodes: int=100,\n",
    "            epochs: int=200,\n",
    "            random_starts: bool=True,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param data: (object) environment data\n",
    "            :param assets: (list) asset names\n",
    "            :param horizon: (int) investment horizon -> max episode time steps\n",
    "            :param window_size: (int) sequence length of the data used for doing prediction\n",
    "                                and estimating variance, covariance etc\n",
    "            :param portfolio_value: (int or float) initial portfolio value\n",
    "            :param risk_aversion: (int or float) constant risk aversion of investor/agent\n",
    "            :param cost_selling, cost_buying: (float) relative cost of selling and buying assets\n",
    "            :param cost_fix: (float) costs for being allowed to trade on each time step\n",
    "            :param predictor: (str) h5 file with prediction model (hyper-)parameters\n",
    "            :param optimized: (bool) for using either optimized or naive weights\n",
    "            :param action_type: (str) how to change weights based on actions:\n",
    "                                'signal', 'signal_softmax', 'direct', 'direct_softmax', 'clipping'\n",
    "            :param action_space: (str) specifies action space -> 'bounded', 'unbounded' or 'discrete'\n",
    "            :param num_actions: (int) number of possible action values for each action\n",
    "                                if action space is discrete\n",
    "            :param scaler: (object) scaler object\n",
    "            :param standardize: (bool) use normalization or standardization for state scaling\n",
    "            :param action_bounds: (tuple) upper and lower bound for continuous actions\n",
    "            :param discrete_states: (bool) true for discrete state space\n",
    "            :param state_labels: (tuple) number of labels for state discretization per state row\n",
    "        \"\"\"\n",
    "        # build logger\n",
    "        #self.logger = get_logger(filename='tmp/env.log', logger_name='EnvLogger')\n",
    "        self.data = data\n",
    "        self.horizon = horizon\n",
    "        self.window_size = window_size\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.init_portfolio_value = portfolio_value\n",
    "        self.optimized = optimized\n",
    "        self.action_type = action_type\n",
    "        self.action_space = action_space\n",
    "        self.num_actions = num_actions\n",
    "        self.action_bounds = action_bounds\n",
    "        self.discrete_states = discrete_states\n",
    "        self.state_labels = state_labels\n",
    "        self.standardize = standardize,\n",
    "        self.episodes = episodes\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.step = 0\n",
    "        self.episode = 0\n",
    "\n",
    "        try:\n",
    "            if assets is None:\n",
    "                self.asset_names = ['A' + str(i + 1) for i in range(nb_assets)]\n",
    "            else:\n",
    "                self.asset_names = assets\n",
    "            if nb_assets is None:\n",
    "                self.nb_assets = len(self.asset_names)\n",
    "            else:\n",
    "                self.nb_assets = nb_assets\n",
    "            if len(self.asset_names) != nb_assets:\n",
    "                self.asset_names = ['A' + str(i + 1) for i in range(nb_assets)]\n",
    "        except Exception as e:\n",
    "            self.logger.error(e)\n",
    "\n",
    "        # build portfolio object\n",
    "        self.portfolio = Portfolio(\n",
    "            portfolio_value=self.init_portfolio_value,\n",
    "            risk_aversion=risk_aversion,\n",
    "            fix_cost=cost_fix,\n",
    "            cost_selling=cost_selling,\n",
    "            cost_buying=cost_buying,\n",
    "            action_type=action_type)\n",
    "\n",
    "        # build data object\n",
    "        self.data_env = DataEnv(\n",
    "            data,\n",
    "            assets,\n",
    "            horizon=horizon,\n",
    "            window_size=window_size,\n",
    "            scaler=scaler,\n",
    "            predictor=predictor,\n",
    "            standardize=standardize,\n",
    "            random_starts=random_starts,\n",
    "            episodes=episodes,\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "        # reset epoch\n",
    "        self.reset_epoch()\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.__class__.__name__)\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        # update current portfolio based on agent action\n",
    "        new_weights, cost, portfolio_value = self.portfolio.update(action)\n",
    "        self.episode_costs += cost\n",
    "\n",
    "        # see PortfolioEnv.execute() for explanation\n",
    "        info = self.data_env.get_window(episode_step=self.step)\n",
    "        self._update(info)\n",
    "\n",
    "        # make a step forward\n",
    "        reward, weights, new_portfolio_value = self.portfolio.get_next_step(self.asset_returns[-1], self.covariance)\n",
    "\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        info = portfolio_info(weights=new_weights,\n",
    "                              old_weights=self.weights,\n",
    "                              new_weights=weights,\n",
    "                              init_weights=self.init_weights,\n",
    "                              asset_returns=self.asset_returns[-1],\n",
    "                              predictions=self.prediction,\n",
    "                              portfolio_value=portfolio_value,\n",
    "                              new_portfolio_value=new_portfolio_value,\n",
    "                              old_portfolio_value=self.portfolio_value,\n",
    "                              portfolio_return=new_portfolio_value / portfolio_value - 1,\n",
    "                              portfolio_variance=self.portfolio.variance,\n",
    "                              sharpe_ratio=self.portfolio.sharpe,\n",
    "                              transaction_costs=cost)\n",
    "\n",
    "        # update portfolio values\n",
    "        self.weights = new_weights\n",
    "        self.portfolio_value = new_portfolio_value\n",
    "        self.portfolio_variance = self.portfolio.variance\n",
    "\n",
    "        # true if episode has finished\n",
    "        done = bool(self.step >= self.horizon)\n",
    "\n",
    "        # update state\n",
    "        self.state = np.concatenate((np.reshape(self.weights, (1, self.nb_assets)), self.__state), axis=0)\n",
    "\n",
    "        # discretize the state array if selected\n",
    "        if self.discrete_states:\n",
    "            state = self._state_discretization()\n",
    "        else:\n",
    "            state = self.state\n",
    "\n",
    "        # flatten the state array and reduce state size -> returns 1d array\n",
    "        state = get_flatten(state)\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        # should not be happening when using a runner\n",
    "        if self.step > self.horizon:\n",
    "            self.reset()\n",
    "\n",
    "        return env_step(state, reward, done, info=info)\n",
    "\n",
    "    def _reset(self):\n",
    "\n",
    "        # is being called after finishing an episode -> get a new episode\n",
    "        self.episode += 1\n",
    "        self.step = 0\n",
    "        self.episode_costs = 0\n",
    "        self.episode_reward = 0\n",
    "\n",
    "        # get new episode start -> get new data observations\n",
    "        self.data_env.reset()\n",
    "        info = self.data_env.get_window()\n",
    "\n",
    "        # update some variables for easier access\n",
    "        self._update(info)\n",
    "\n",
    "        # get a new episode start\n",
    "        self.entry_point = self.data_env.episode_start\n",
    "\n",
    "        self.logger.debug(f'{20 * \"#\"} Episode {self.episode} {50 * \"#\"}')\n",
    "        self.logger.debug(f'Episode entry point:{self.entry_point}')\n",
    "\n",
    "        # estimate initial weights for the assets (optimized or naive)\n",
    "        self.init_weights = self._get_init_weights(self.asset_returns, self.covariance, optimized=self.optimized)\n",
    "        self.weights = self.init_weights\n",
    "\n",
    "        # reset the portfolio\n",
    "        self.portfolio.reset(covariance=self.covariance, weights=self.weights)\n",
    "        self.portfolio_value = self.portfolio.init_portfolio_value\n",
    "        self.portfolio_variance = self.portfolio.variance\n",
    "\n",
    "        self.logger.debug(f'Initial weights for episode {self.episode}:\\n{self.weights}')\n",
    "\n",
    "        # returns the initial episode environment state\n",
    "        self.state = np.concatenate((np.reshape(self.weights, (1, self.nb_assets)), self.__state), axis=0)\n",
    "\n",
    "        # discretize the state array if selected\n",
    "        if self.discrete_states:\n",
    "            state = self._state_discretization()\n",
    "        else:\n",
    "            state = self.state\n",
    "\n",
    "        # flatten the array -> returns a 1d array\n",
    "        state = get_flatten(state)\n",
    "        return state\n",
    "\n",
    "    def _get_init_weights(self, asset_returns, covariance, optimized=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param asset_returns: (object) past asset returns\n",
    "            :param covariance: (object) covariance matrix\n",
    "            :param optimized: (bool) for doing optimization\n",
    "        :return: semi-optimized weights based on past mean variance or naive 1/n weights\n",
    "        \"\"\"\n",
    "        return WeightOptimize(asset_returns, covariance, nb_assets=self.nb_assets,\n",
    "                              risk_aversion=self.risk_aversion).optimize_weights(method='SLSQP') \\\n",
    "            if optimized else [1/self.nb_assets for _ in range(self.nb_assets)]\n",
    "\n",
    "    def _update(self, info):\n",
    "        self.__state = info.state\n",
    "        self.window = info.window\n",
    "        self.asset_mean = info.mean\n",
    "        self.asset_variance = info.variance\n",
    "        self.asset_returns = info.asset_returns\n",
    "        self.correlation = info.correlation\n",
    "        self.covariance = info.covariance\n",
    "        self.prediction = info.prediction\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Close environment. No other method calls possible afterwards.\n",
    "        Currently not implemented.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reset_epoch(self):\n",
    "        # is called at the start for each new epoch\n",
    "        self.episode = 0\n",
    "        return self._reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # is called after an episode has finished\n",
    "        self.logger.debug(f'Episode {self.episode} has finished.'\n",
    "                          f'\\nCumulative Reward:{self.episode_reward}'\n",
    "                          f'\\nPortfolio Value: {self.portfolio_value}'\n",
    "                          f'\\nPortfolio Return: {self.portfolio.portfolio_return}'\n",
    "                          f'\\nPortfolio Variance: {self.portfolio.variance}'\n",
    "                          f'\\nSharpe Ration: {self.portfolio.sharpe}\\n')\n",
    "        return self._reset()\n",
    "\n",
    "    def execute(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param action: (list) agent actions to execute\n",
    "        \"\"\"\n",
    "        return self._step(action)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        return np.random.seed(seed)\n",
    "\n",
    "    def _state_discretization(self, nb_labels=(50,)):\n",
    "        if len(nb_labels) == 1:\n",
    "            nb_labels = [nb_labels[0] for _ in range(self.state.shape[0])]\n",
    "\n",
    "        # quantil based state discretization\n",
    "        # TODO: some issues with tensorforce agent NNs (expects float32 gets int32...), maybe wait for an update\n",
    "        _state = np.array([pd.qcut(self.state[i], nb_labels[i], labels=False, duplicates='drop')\n",
    "                           for i in range(self.state.shape[0])], dtype='int32')\n",
    "        return _state\n",
    "\n",
    "    def env_spec(self):\n",
    "        # returns a dict of environment configurations\n",
    "        return dict(\n",
    "            data_shape=self.data.shape,\n",
    "            epochs=self.epochs,\n",
    "            episodes=self.episodes,\n",
    "            horizon=self.horizon,\n",
    "            window_size=self.window_size,\n",
    "            portfolio_value=self.init_portfolio_value,\n",
    "            risk_aversion=self.risk_aversion,\n",
    "            optimized_weights=self.optimized,\n",
    "            action_type=self.action_type,\n",
    "            action_space=self.action_space,\n",
    "            discrete_states=self.discrete_states,\n",
    "            standardize=self.standardize,\n",
    "            num_actions=self.num_actions\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def states(self):\n",
    "        \"\"\"\n",
    "        Return the state space.\n",
    "        Returns: dict of state properties (shape and type).\n",
    "        => weight + scaled mean + scaled variance + scaled predictions + correlation\n",
    "        \"\"\"\n",
    "\n",
    "        if self.discrete_states:\n",
    "            # returns discrete state space shape\n",
    "            return dict(shape=(0.5 * self.nb_assets * (self.nb_assets + 7),), type='int')\n",
    "        else:\n",
    "            # returns continuous state space shape\n",
    "            return dict(shape=(0.5 * self.nb_assets * (self.nb_assets + 7),), type='float')\n",
    "\n",
    "    @property\n",
    "    def actions(self):\n",
    "        \"\"\"\n",
    "        Return the action space.\n",
    "        Returns: dict of action properties (continuous, number of actions)\n",
    "        \"\"\"\n",
    "        # discrete action space -> categorical distribution\n",
    "        if self.action_space == 'discrete':\n",
    "            return dict(shape=(self.nb_assets,), num_actions=self.num_actions, type='int')\n",
    "\n",
    "        # continuous action space with upper and lower bounds -> beta distribution\n",
    "        elif self.action_space == 'bounded':\n",
    "            return dict(shape=(self.nb_assets,), type='float',\n",
    "                        min_value=self.action_bounds[0], max_value=self.action_bounds[1])\n",
    "\n",
    "        # unbounded continuous action space (default) -> gaussian distribution\n",
    "        else:\n",
    "            return dict(shape=(self.nb_assets,), type='float')\n",
    "\n",
    "\n",
    "class Env(object):\n",
    "\n",
    "    def _step(self, action):\n",
    "\n",
    "        # update current portfolio based on agent action\n",
    "        new_weights, cost, portfolio_value = self.portfolio.update(action)\n",
    "        benchmark_weights = self.weigths\n",
    "        self.episode_costs += cost\n",
    "\n",
    "        # see PortfolioEnv.execute() for explanation\n",
    "        info = self.data_env.get_window(episode_step=self.step)\n",
    "        self._update(info)\n",
    "\n",
    "        # make a step forward\n",
    "        reward, weights, new_portfolio_value = self.portfolio.get_next_step(self.asset_returns[-1], self.covariance)\n",
    "\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        info = portfolio_info(weights=new_weights,\n",
    "                              old_weights=self.weights,\n",
    "                              new_weights=weights,\n",
    "                              init_weights=self.init_weights,\n",
    "                              asset_returns=self.asset_returns[-1],\n",
    "                              predictions=self.prediction,\n",
    "                              portfolio_value=portfolio_value,\n",
    "                              new_portfolio_value=new_portfolio_value,\n",
    "                              old_portfolio_value=self.portfolio_value,\n",
    "                              portfolio_return=new_portfolio_value / portfolio_value - 1,\n",
    "                              portfolio_variance=self.portfolio.variance,\n",
    "                              sharpe_ratio=self.portfolio.sharpe,\n",
    "                              transaction_costs=cost)\n",
    "\n",
    "        # update portfolio values\n",
    "        self.weights = new_weights\n",
    "        self.portfolio_value = new_portfolio_value\n",
    "        self.portfolio_variance = self.portfolio.variance\n",
    "\n",
    "        # update state\n",
    "        self.state = np.concatenate((np.reshape(self.weights, (1, self.nb_assets)), self.__state), axis=0)\n",
    "\n",
    "        # flatten the state array and reduce state size -> returns 1d array\n",
    "        state = get_flatten(state)\n",
    "\n",
    "        # estimate benchmark = buy and hold value\n",
    "        benchmark = pass\n",
    "\n",
    "        self.step += 1\n",
    "\n",
    "        # should not be happening when using a runner\n",
    "        if self.step > self.horizon:\n",
    "            self.reset()\n",
    "\n",
    "        return env_step(state, reward, benchmark, info=info)\n",
    "\n",
    "    def execute(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            :param action: (list) agent actions to execute\n",
    "        \"\"\"\n",
    "        return self._step(action)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def action(self):\n",
    "        return self.nb_assets\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        # => weight + scaled mean + scaled variance + scaled predictions + correlation\n",
    "        return 0.5 * self.nb_assets * (self.nb_assets + 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self, data: np.ndarray, seed: int):\n",
    "        self.data = data\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def observation(self):\n",
    "        NotImplementedError\n",
    "        \n",
    "    def step(self, action: np.ndarray):\n",
    "        reward, info = self._step(action)\n",
    "        return reward, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return self._reset()\n",
    "    \n",
    "    def _step(self, action):\n",
    "        NotImplementedError\n",
    "    \n",
    "    def _reset(self):\n",
    "        NotImplementedError\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(self.__class__.__name__)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        NotImplementedError\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        NotImplementedError\n",
    "        \n",
    "\n",
    "class PortfolioEnv(Env):\n",
    "    def __init__(\n",
    "        self, \n",
    "        data: np.ndarray,\n",
    "        nb_assets: int = 8,\n",
    "        episodes: int = 100,\n",
    "        horizon: int = 30,\n",
    "        window_size: int = 100,\n",
    "        portfolio_value: float = 1000.,\n",
    "        risk_aversion: float = 1.,\n",
    "        costs: float = 0.025,\n",
    "        seed: int = 42\n",
    "    ):\n",
    "        super(PortfolioEnv, self).__init__(data, seed)\n",
    "        self.nb_assets = nb_assets\n",
    "        #self.horizon = horizon\n",
    "        #self.window_size = window_size\n",
    "        self.portfolio_value = portfolio_value\n",
    "        self.risk_aversion = risk_aversion\n",
    "        self.costs = costs\n",
    "        self.dl = DataLoader(data, nb_assets, episodes, horizon, window_size)\n",
    "        \n",
    "    def observation(self):\n",
    "        weights, var_covar, returns, mean, areturn = ()\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config_spec(cls, data, mode=\"train\"):\n",
    "        import pgtaa.config as cfg  \n",
    "        if mode == \"train\":\n",
    "            episodes = cfg.TRAIN_EPISODES\n",
    "        else:\n",
    "            episodes = cfg.TEST_EPISODES\n",
    "        return cls(data, cfg.NB_ASSETS, episodes, cfg.HORIZON, cfg.WINDOW_SIZE, \n",
    "                   cfg.PORTFOLIO_INIT_VALUE, cfg.RISK_AVERSION, cfg.COSTS, cfg.SEED)\n",
    "        \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self.nb_assets,\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return int(0.5 * self.nb_assets * (self.nb_assets + 7)),\n",
    "    \n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, \n",
    "                 data: np.ndarray, \n",
    "                 nb_assets: int, \n",
    "                 episodes: int,\n",
    "                 horizon: int, \n",
    "                 window_size: int\n",
    "                ):\n",
    "        \n",
    "        self.data = data\n",
    "        self.horizon = horizon\n",
    "        self.nb_assets =nb_assets\n",
    "        self.episodes = episodes\n",
    "        self.window_size = window_size\n",
    "        \n",
    "    def init_batches():\n",
    "        \n",
    "    def get_batch(self):\n",
    "        print(self.episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(TRAIN_CSV)\n",
    "p = PortfolioEnv.from_config_spec(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the state space can be calculated by $\\frac{1}{2}n(n+7)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fast1]",
   "language": "python",
   "name": "conda-env-fast1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
